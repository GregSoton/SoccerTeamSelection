{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb04c72e-c60a-49de-93e9-f08de432b51c",
   "metadata": {},
   "source": [
    "# Injury Probability Model - Paper Section 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed66cb0-9c69-4f73-ab5f-ab666c7a9ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import pickle\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, cross_val_score, cross_val_predict, StratifiedShuffleSplit\n",
    "import xgboost as xgb\n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade3d22-8ccb-4e56-b97a-26083277b6e7",
   "metadata": {},
   "source": [
    "### Load Data - Preprocessed in other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1185768c-975c-4178-bcfc-e2bd4856e516",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('data/injury_data/injury_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0323a8-c14b-4b55-a63c-0197ac5b3f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[['team_id','goal_diff','mins_played', 'matches_played', 'num_tackles', 'num_fouls', 'num_bad_touches', 'num_ball_touches', 'num_dribbles',\\\n",
    "       'num_tackleds', 'num_fouleds', 'days_since_last_game', 'dist_covered', 'metres_per_min', 'hir_dist',\\\n",
    "       'sprint_dist', 'num_hirs', 'num_sprints', 'accels', 'decels',\\\n",
    "       'LI_accels', 'LI_decels', 'acute_workload', 'chronic_workload', 'ACWR', 'num_injuries', 'total_days_out','total_games_missed',\\\n",
    "       'days_out_last_injury','frequency_most_prominent_injury', 'days_out_most_prominent_injury',\\\n",
    "       'days_out_most_serious_injury','injuries_past_three_months',\\\n",
    "       'injuries_past_six_months', 'injuries_past_twelve_months','opp_num_tackles', 'opp_num_fouls','opp_num_bad_touches', 'opp_num_touches',\\\n",
    "       'opp_num_dribbles','opp_num_times_tackled', 'opp_num_times_fouled','opp_matches_played', 'opp_days_since_last_game','attendance',\\\n",
    "       'temp', 'snow', 'windspeedMiles',\\\n",
    "       'weatherCode', 'precipMM', 'humidity', 'visibility', 'pressure',\\\n",
    "       'distance', 'height', 'age', 'rolling_mins_played_exp','days_diff','rolling_days_diff_exp','opp_team_id']]\n",
    "\n",
    "#X = data[['num_injuries','total_days_out','precipMM','temp','injuries_past_twelve_months','injuries_past_three_months']]\n",
    "X['total_days_out'] = (X['total_days_out'] / X['num_injuries']).fillna(0)\n",
    "\n",
    "X_team = data['team_id']\n",
    "X_opp_team = data['opp_team_id']\n",
    "y = data['injured'].astype(int)\n",
    "X_train_df, X_test_df, y_train, y_test = train_test_split(X, y, test_size=0.25, shuffle=False, random_state=1)\n",
    "ratio = sum(y_train) / len(y_train)\n",
    "ratio = np.full(len(y_test),ratio) \n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,RobustScaler,MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_df)\n",
    "X_test_scaled = scaler.transform(X_test_df)\n",
    "X_train = pd.DataFrame(X_train_scaled, index=X_train_df.index, columns=X_train_df.columns)\n",
    "X_test = pd.DataFrame(X_test_scaled, index=X_test_df.index, columns=X_test_df.columns)\n",
    "teams = X_team.unique()\n",
    "num_teams = len(teams)\n",
    "\n",
    "def team_ohe(teams,team_id):\n",
    "    return teams==team_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e672dc8b-37ea-4d0a-b5fd-13e2d397bfe6",
   "metadata": {},
   "source": [
    "### Train and save the Injury Model w/ Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35807ad-33f0-4e23-adfa-60803d11f5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_kfold = data[['team_id','dist_covered', 'acute_workload', 'chronic_workload', 'num_injuries', 'total_days_out','num_dribbles',\\\n",
    "       'days_out_last_injury','frequency_most_prominent_injury', 'days_out_most_prominent_injury',\\\n",
    "       'days_out_most_serious_injury','days_since_last_injury',\\\n",
    "       'injuries_past_twelve_months','opp_num_tackles', 'opp_num_fouls','temp', 'precipMM',\n",
    "       'distance', 'age', 'opp_team_id']]\n",
    "\n",
    "X_kfold_wo_nans = X_kfold.fillna(1000)\n",
    "#X_kfold[['team_id','opp_team_id']] = X_kfold[['team_id','opp_team_id']].astype(\"category\")\n",
    "#X_kfold_wo_nans[['team_id','opp_team_id']] = X_kfold_wo_nans[['team_id','opp_team_id']].astype(\"category\")\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "preds_all = np.array([])\n",
    "indexes = np.array([])\n",
    "model_log_losses = []\n",
    "model_cis = []\n",
    "\n",
    "classifiers = [\n",
    "    #KNeighborsClassifier(1000),\n",
    "    #LogisticRegression(max_iter=1000,C=0.2),\n",
    "    #RandomForestClassifier(max_depth=4, n_estimators=300,min_samples_leaf=1),\n",
    "    XGBClassifier(tree_method='hist', objective='binary:logistic', max_depth=4, n_estimators=400,eta=0.025,colsample_bytree=0.3,colsample_bylevel=0.3, alpha=10)\n",
    "    #GradientBoostingClassifier(n_estimators=400,max_depth=4,learning_rate=learning_rate)\n",
    "    #MLPClassifier(alpha=1, max_iter=1000)\n",
    "]\n",
    "\n",
    "names = [\n",
    "    #\"Nearest Neighbors\",\n",
    "    #\"Logistic Regression\",\n",
    "    #\"Random Forest\",\n",
    "    \"XGBoost\",\n",
    "    #\"Neural Net\",\n",
    "]\n",
    "\n",
    "param = {'tree_method':'hist', 'objective':'binary:logistic', 'max_depth':4, 'n_estimators':400,'eta':0.025,'colsample_bytree':0.3,'colsample_bylevel':0.3, 'alpha':10}\n",
    "\n",
    "prediction_dfs = []\n",
    "train_prediction_dfs = []\n",
    "\n",
    "index_sets = []\n",
    "train_index_sets = []\n",
    "for j in range(len(names)):\n",
    "    print(\"Name: \", names[j])\n",
    "    log_losses= []\n",
    "    train_log_losses = []\n",
    "    ratio_lls = []\n",
    "    for i, (train_index, test_index) in enumerate(kfold.split(X_kfold, y)):\n",
    "        print(\"Fold: \", i)\n",
    "        index_sets.append(test_index)\n",
    "        train_index_sets.append(train_index)\n",
    "        indexes = np.append(indexes,test_index)\n",
    "        scaler = StandardScaler()\n",
    "        if names[j] == 'XGBoost':\n",
    "            X_ktrain = X_kfold.iloc[train_index] \n",
    "            X_ktest = X_kfold.iloc[test_index] \n",
    "        else:\n",
    "            X_ktrain = X_kfold_wo_nans.iloc[train_index] \n",
    "            X_ktest = X_kfold_wo_nans.iloc[test_index] \n",
    "        X_ktrain = scaler.fit_transform(X_ktrain)\n",
    "        X_ktest = scaler.transform(X_ktest)\n",
    "        filename = 'injury_scaler_efficient.sav'\n",
    "        pickle.dump(scaler, open(filename, 'wb'))\n",
    "        y_ktrain = y.iloc[train_index]\n",
    "        y_ktest = y.iloc[test_index]\n",
    "        evalset = [(X_ktrain, y_ktrain), (X_ktest,y_ktest)]\n",
    "        #model = classifiers[j]#KNeighborsClassifier(1000)#LogisticRegression()#XGBClassifier(objective='binary:logistic', max_depth=4, n_estimators=250,learning_rate=0.025,colsample_bytree=0.2)\n",
    "        #model.fit(X_ktrain, y_ktrain)\n",
    "        dtrain = xgb.DMatrix(X_ktrain,label=y_ktrain)\n",
    "        dtest = xgb.DMatrix(X_ktest,label=y_ktest)\n",
    "        model = xgb.train(param,dtrain,400,[(dtest,'eval'),(dtrain,'train')])\n",
    "        filename = 'injury_model_efficient.sav'\n",
    "        pickle.dump(model, open(filename, 'wb'))\n",
    "        preds = model.predict(dtest)\n",
    "        preds_training = model.predict(dtrain)\n",
    "        print(preds)\n",
    "        #preds_all = np.append(preds_all, preds[:,1].flatten())\n",
    "        preds_all = np.append(preds_all, preds)\n",
    "        ratio = np.full(len(y_ktest),y_ktrain.sum()/len(y_ktrain))\n",
    "        print(\"Ratio: \", log_loss(y_ktest,ratio))\n",
    "        ratio_lls.append(log_loss(y_ktest,ratio))\n",
    "        print(log_loss(y_ktest, preds))\n",
    "        log_losses.append(log_loss(y_ktest, preds))\n",
    "        train_log_losses.append(log_loss(y_ktrain,preds_training))\n",
    "        pred_eval_df = pd.DataFrame([])\n",
    "        #pred_eval_df['prob'] = preds[:,1].flatten()\n",
    "        pred_eval_df['prob'] = preds\n",
    "        #pred_eval_df = pred_eval_df.sort_values('index')\n",
    "        pred_eval_df['injured'] = y_ktest.reset_index(drop=True)\n",
    "        pred_eval_df.index = test_index\n",
    "\n",
    "        train_pred_eval_df = pd.DataFrame([])\n",
    "        #train_pred_eval_df['prob'] = preds_training[:,1].flatten()\n",
    "        train_pred_eval_df['prob'] = preds_training\n",
    "        train_pred_eval_df['injured'] = y_ktrain.reset_index(drop=True)\n",
    "        train_pred_eval_df.index = train_index\n",
    "        prediction_dfs.append(pred_eval_df)\n",
    "        train_prediction_dfs.append(train_pred_eval_df)\n",
    "print(\"Mean training log loss: \", np.array(train_log_losses).mean())\n",
    "print(\"Mean log loss: \", np.array(log_losses).mean())  \n",
    "print(\"STD log loss: \", np.array(log_losses).std())  \n",
    "model_log_losses.append(np.array(log_losses).mean())\n",
    "model_cis.append((np.array(log_losses).std() / np.sqrt(10))*1.650)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247d3c3b-7cbd-461e-9f2a-ce11ccff5b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data/predictions/injury_predictions_XGB.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f421af-150f-4595-80fc-7e93fec89ea8",
   "metadata": {},
   "source": [
    "## Shap Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf65793-f10c-4101-9ff3-7554358c6a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "xgboost_model = XGBClassifier(tree_method=\"gpu_hist\", objective='binary:logistic', max_depth=4, n_estimators=300,eta=0.04,colsample_bytree=0.1,colsample_bylevel=0.3, alpha=10, enable_categorical=True)\n",
    "xgboost_model.fit(X_ktrain, y_ktrain)\n",
    "preds = xgboost_model.predict_proba(X_ktest)\n",
    "\n",
    "def shap_explainer(inj_model,is_tree,train_x,train_y,test_x,index,columns):\n",
    "    if is_tree:\n",
    "        explainer = shap.TreeExplainer(inj_model, model_output='probability', feature_dependence='dependent', data=train_x,feature_names=columns)\n",
    "        shap_values = explainer.shap_values(test_x)\n",
    "    else:\n",
    "        explainer = shap.Explainer(inj_model, train_x, feature_names=columns,model_output=\"probability\")\n",
    "        shap_values = explainer.shap_values(test_x)\n",
    "    shap.initjs()\n",
    "    display(shap.force_plot(explainer.expected_value, shap_values, scaler.inverse_transform(test_x[index:index+1000]),feature_names=columns))\n",
    "    shap_df = pd.DataFrame(data=[(shap_values[0]*100).round(3),scaler.inverse_transform(X_ktest[index:index+1]).flatten().round(1)], columns=columns, index=['% Contribution','Value']).transpose().sort_values(by='% Contribution', ascending=True)\n",
    "\n",
    "    print(\"Prob: \", shap_df['% Contribution'].sum().round(4),\"%\")\n",
    "    print(\"Base value: \", (explainer.expected_value*100).round(3), \"%\")\n",
    "    print(\"Total prob: \", (explainer.expected_value*100 + (shap_df['% Contribution'].sum().round(4))).round(3), \"%\")\n",
    "    return shap_df, shap_values,explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced0d303-474f-447f-8e59-bf5ec79cc343",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_df, shap_values,explainer = shap_explainer(xgboost_model, True, X_ktrain, y_ktrain, X_ktest,155, X_kfold.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f89473-b46c-4cb1-aa3e-bf73ff03ff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "shap.summary_plot(explainer(X_ktest), plot_type=\"bar\", show=False,plot_size=[10,8])\n",
    "\n",
    "# Show the plot\n",
    "plt.xlabel('Mean |SHAP| (average impact on model output)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
